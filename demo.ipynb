{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb_extension.DuckDBPyConnection at 0x1054014f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamming Space search\n",
    "\n",
    "This is a demonstration \n",
    "\n",
    "# Step 1: Load Data\n",
    "\n",
    "First, we create a local database using duckdb. This isn't strictly necessary: if you want to save some disk space, you can change `CREATE TABLE` below to `CREATE VIEW`, which will simply scan the parquet files in place rather than fill them into a new database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"database.db\")\n",
    "con.execute(\"CREATE TABLE hamming AS SELECT * FROM parquet_scan('parquet/*.parquet')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step two: set up a hashing instance.\n",
    "\n",
    "Now we need to build a hasher using SRP. You should be able to install the SRP package using `pip install pysrp`. The SRP hasher does basic whitespace tokenization, which works pretty well for languages with spaces (i.e. not Chinese, Thai, and a few others) and turns text into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SRP\n",
    "hasher = SRP.SRP(1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 \n",
    "\n",
    "Get some text. \n",
    "\n",
    "Here I grab an arbitrary e-text from Project Gutenberg. Anything should work here--load from your disk, whatever. The only rules are:\n",
    "\n",
    "1. It should be unicode\n",
    "2. It should be kind of long: at least a couple hundred words, and ideally a hundred thousand words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "text = strip_headers(load_etext(271)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: create a vectorized text.\n",
    "\n",
    "Now we need to use SRP to hash this. I do a couple numpy tricks here: first I do a boolean test on each dimension to test if it's over zero; \n",
    "then I take a view of that as a numpy 64-bit integer. That number is only every going to be used as a string of bytes, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rep_floating = hasher.stable_transform(text, unit_length=True)\n",
    "packed = np.packbits(rep_floating > 0).view(np.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Turn the bytes into a single-row database table.\n",
    "\n",
    "This is pretty straightforward: we just clone the basic database structure, and then insert a new row.\n",
    "\n",
    "Note that you could have as many elements as you wanted in this query table; you could even do fancy weighting where a text\n",
    "should be close to some documents and far from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb_extension.DuckDBPyConnection at 0x1054014f0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS query\")\n",
    "con.execute(f\"CREATE TABLE query AS SELECT * FROM hamming LIMIT 1\")\n",
    "con.execute(f\"DELETE FROM query\")\n",
    "\n",
    "q = f'''INSERT INTO query VALUES ('NA', {\", \".join([str(p) for p in packed])})'''\n",
    "con.execute(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Create a list of searches.\n",
    "\n",
    "This is a little complicated, because we are doing the Hamming distance on each of the twenty integers: I write a function here to create the bit that will test how far about the query is from the searched texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BIT_COUNT(xor(l.A, r.A))::FLOAT + BIT_COUNT(xor(l.B, r.B))::FLOAT + BIT_COUNT(xor(l.C, r.C))::FLOAT + BIT_COUNT(xor(l.D, r.D))::FLOAT + BIT_COUNT(xor(l.E, r.E))::FLOAT + BIT_COUNT(xor(l.F, r.F))::FLOAT + BIT_COUNT(xor(l.G, r.G))::FLOAT + BIT_COUNT(xor(l.H, r.H))::FLOAT + BIT_COUNT(xor(l.I, r.I))::FLOAT + BIT_COUNT(xor(l.J, r.J))::FLOAT + BIT_COUNT(xor(l.K, r.K))::FLOAT + BIT_COUNT(xor(l.L, r.L))::FLOAT + BIT_COUNT(xor(l.M, r.M))::FLOAT + BIT_COUNT(xor(l.N, r.N))::FLOAT + BIT_COUNT(xor(l.O, r.O))::FLOAT + BIT_COUNT(xor(l.P, r.P))::FLOAT + BIT_COUNT(xor(l.Q, r.Q))::FLOAT + BIT_COUNT(xor(l.R, r.R))::FLOAT + BIT_COUNT(xor(l.S, r.S))::FLOAT + BIT_COUNT(xor(l.T, r.T))::FLOAT'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def dist_clause():\n",
    "  \"\"\"\n",
    "  return text of the formula to calculate hamming distance across the 20 chunks of data I provide in the files.\n",
    "  \"\"\"\n",
    "  funcs = []\n",
    "  for i in range(20):\n",
    "    letter = chr(i + 65)\n",
    "    func = f\"BIT_COUNT(xor(l.{letter}, r.{letter}))::FLOAT\"\n",
    "    funcs.append(func)\n",
    "  return f\" {' + '.join(funcs)}\"\n",
    "\n",
    "dist_clause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can actually run the query, finding the fifty closest books to our seed text. This takes about 3 seconds to scan across 20 million books on my laptop. Not terrible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = con.query(f'''\n",
    "  SELECT SUM({dist_clause()}) AS distance , l.htid AS htid FROM hamming AS l CROSS JOIN query AS r GROUP BY l.htid ORDER BY distance  ASC LIMIT 50\n",
    "''')\n",
    "\n",
    "entries = q.arrow()['htid']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see what these ids are by checking them in the hathi trust bibliographic API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black Beauty; the autobiography of a horse (njp.32101025286947)\n",
      "Black Beauty (nc01.ark:/13960/t8nc72h97)\n",
      "Black Beauty : his groom and companions. The \"Uncle Tom's cabin\" of the horse (hvd.hwp3ii)\n",
      "Black Beauty : the autobiography of a horse (mdp.39015038609189)\n",
      "Black Beauty : the autobiography of a horse (wu.89092951672)\n",
      "Black Beauty, the autobiography of a horse (mdp.39015011405985)\n",
      "Black Beauty :  the autobiography of a horse (pst.000026473765)\n",
      "Black Beauty (uva.x001134706)\n",
      "Black beauty (nc01.ark:/13960/t3jw9gh8b)\n",
      "Black Beauty, his groom and companions. : The \"Uncle Tom's cabin\" of the horse (mdp.39015040765490)\n",
      "Black Beauty (mdp.39076000504394)\n",
      "Black Beauty : the autobiography of a horse (mmet.ark:/13960/t8z89k129)\n",
      "Black beauty (uc2.ark:/13960/t2b853x3w)\n",
      "Black Beauty : the autobiography of a horse (mdp.39076002790348)\n",
      "Black Beauty; the autobiography of a horse (hvd.hwp3ij)\n",
      "Black Beauty : his grooms and companions ; the autobiography of a horse (uc2.ark:/13960/t4qj79244)\n",
      "Black Beauty (inu.32000000654352)\n",
      "Black Beauty (pst.000046825032)\n",
      "Black Beauty : his grooms and companions (mdp.39015031312039)\n",
      "Black Beauty : the autobiography of a horse (pst.000020897567)\n"
     ]
    }
   ],
   "source": [
    "for entry in entries[:20]:\n",
    "  title = [*requests.get(f\"https://catalog.hathitrust.org/api/volumes/brief/htid/{entry}.json\").json()['records'].items()][0][1]['titles'][0]\n",
    "  print(f\"{title} ({entry})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you do with these matches is up to you: I recommend the [HTRC Feature Reader Library](https://github.com/htrc/htrc-feature-reader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import Volume\n",
    "closest = Volume(entries.to_pylist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/njp.32101025286947'>Black Beauty; the autobiography of a horse,</a></strong> by <em>Sewell, Anna, 1820-1878.</em> (1911, 316 pages) - <code>njp.32101025286947</code>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x14fb60550>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th>section</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <th>body</th>\n",
       "      <th>At</th>\n",
       "      <th>IN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <th>body</th>\n",
       "      <th>on</th>\n",
       "      <th>IN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <th>body</th>\n",
       "      <th>I</th>\n",
       "      <th>CD</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <th>body</th>\n",
       "      <th>fits</th>\n",
       "      <th>NNS</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <th>body</th>\n",
       "      <th>the</th>\n",
       "      <th>DT</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <th>body</th>\n",
       "      <th>``</th>\n",
       "      <th>``</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <th>body</th>\n",
       "      <th>health</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <th>body</th>\n",
       "      <th>bleed</th>\n",
       "      <th>VB</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <th>body</th>\n",
       "      <th>you</th>\n",
       "      <th>PRP</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <th>body</th>\n",
       "      <th>something</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count\n",
       "page section token     pos       \n",
       "109  body    At        IN       1\n",
       "176  body    on        IN       1\n",
       "211  body    I         CD       1\n",
       "124  body    fits      NNS      1\n",
       "69   body    the       DT       1\n",
       "242  body    ``        ``       3\n",
       "267  body    health    NN       1\n",
       "39   body    bleed     VB       1\n",
       "155  body    you       PRP      2\n",
       "285  body    something NN       1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest.tokenlist().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4203a5cb41a9b3ea84e99fe66c07097d8c9544709fbc84b9fa75e017137c69a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
